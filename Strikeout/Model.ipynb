{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (20000,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 52\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Specify the path to your annotations directory\u001b[39;00m\n\u001b[0;32m     51\u001b[0m annotations_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSentence_Dataset/annotations\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 52\u001b[0m encoded_annotations, character_map \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mannotations_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 46\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(annotations_dir)\u001b[0m\n\u001b[0;32m     44\u001b[0m annotations \u001b[38;5;241m=\u001b[39m load_annotations(annotations_dir)\n\u001b[0;32m     45\u001b[0m char_map \u001b[38;5;241m=\u001b[39m create_char_map(annotations)\n\u001b[1;32m---> 46\u001b[0m encoded_annotations \u001b[38;5;241m=\u001b[39m \u001b[43mencode_annotations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mannotations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchar_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample encoded annotations:\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoded_annotations[:\u001b[38;5;241m5\u001b[39m])\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m encoded_annotations, char_map\n",
      "Cell \u001b[1;32mIn[17], line 40\u001b[0m, in \u001b[0;36mencode_annotations\u001b[1;34m(annotations, char_map)\u001b[0m\n\u001b[0;32m     38\u001b[0m     encoded_annotation\u001b[38;5;241m.\u001b[39mpop()  \u001b[38;5;66;03m# Remove the last '#' for the end of the sentence\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     encoded_data\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39marray(encoded_annotation))\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (20000,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "def load_annotations(annotations_dir):\n",
    "    \"\"\" Load all annotations from the directory and return a list of strings. \"\"\"\n",
    "    all_annotations = []\n",
    "    for filename in os.listdir(annotations_dir):\n",
    "        with open(os.path.join(annotations_dir, filename), 'r', encoding='utf-8') as file:\n",
    "            all_annotations.append(file.read().strip())\n",
    "    return all_annotations\n",
    "\n",
    "def create_char_map(annotations):\n",
    "    \"\"\" Create a character map from the annotations. \"\"\"\n",
    "    unique_chars = set()\n",
    "    for annotation in annotations:\n",
    "        chars = annotation.replace(\"#\", \" \").replace(\"\\\\\", \"\").split()\n",
    "        for char in chars:\n",
    "            unique_chars.update(char)\n",
    "    # Add a character for word separator\n",
    "    unique_chars.add(\"#\")\n",
    "    # Sort characters to maintain consistency\n",
    "    sorted_chars = sorted(list(unique_chars))\n",
    "    char_to_int = {char: i + 1 for i, char in enumerate(sorted_chars)}  # Start indexing from 1\n",
    "    char_to_int[\" \"] = 0  # Use 0 for space (used as padding)\n",
    "    return char_to_int\n",
    "\n",
    "def encode_annotations(annotations, char_map):\n",
    "    \"\"\" Encode annotations into sequences of integers suitable for training. \"\"\"\n",
    "    encoded_data = []\n",
    "    for annotation in annotations:\n",
    "        encoded_annotation = []\n",
    "        words = annotation.split(\"#\")\n",
    "        for word in words:\n",
    "            for char in word:\n",
    "                if char in char_map:\n",
    "                    encoded_annotation.append(char_map[char])\n",
    "            encoded_annotation.append(char_map[\"#\"])  # Add word separator\n",
    "        encoded_annotation.pop()  # Remove the last '#' for the end of the sentence\n",
    "        encoded_data.append(np.array(encoded_annotation))\n",
    "    return np.array(encoded_data)\n",
    "\n",
    "\n",
    "def main(annotations_dir):\n",
    "    annotations = load_annotations(annotations_dir)\n",
    "    char_map = create_char_map(annotations)\n",
    "    encoded_annotations = encode_annotations(annotations, char_map)\n",
    "    print(\"Sample encoded annotations:\", encoded_annotations[:5])\n",
    "    return encoded_annotations, char_map\n",
    "\n",
    "# Specify the path to your annotations directory\n",
    "annotations_dir = \"Sentence_Dataset/annotations\"\n",
    "encoded_annotations, character_map = main(annotations_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'#': 1,\n",
       " '-': 2,\n",
       " 'क': 3,\n",
       " 'ख': 4,\n",
       " 'ग': 5,\n",
       " 'घ': 6,\n",
       " 'च': 7,\n",
       " 'छ': 8,\n",
       " 'ज': 9,\n",
       " 'ञ': 10,\n",
       " 'ट': 11,\n",
       " 'ठ': 12,\n",
       " 'ड': 13,\n",
       " 'ढ': 14,\n",
       " 'ण': 15,\n",
       " 'त': 16,\n",
       " 'थ': 17,\n",
       " 'द': 18,\n",
       " 'ध': 19,\n",
       " 'न': 20,\n",
       " 'प': 21,\n",
       " 'फ': 22,\n",
       " 'ब': 23,\n",
       " 'भ': 24,\n",
       " 'म': 25,\n",
       " 'य': 26,\n",
       " 'र': 27,\n",
       " 'ल': 28,\n",
       " 'व': 29,\n",
       " 'श': 30,\n",
       " 'ष': 31,\n",
       " 'स': 32,\n",
       " 'ह': 33,\n",
       " '़': 34,\n",
       " '्': 35,\n",
       " '०': 36,\n",
       " '१': 37,\n",
       " '२': 38,\n",
       " '३': 39,\n",
       " '४': 40,\n",
       " '५': 41,\n",
       " '६': 42,\n",
       " '७': 43,\n",
       " '८': 44,\n",
       " '९': 45,\n",
       " ' ': 0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1632"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def find_max_width(image_dir):\n",
    "    \"\"\" Find the maximum width of images in the specified directory. \"\"\"\n",
    "    max_width = 0\n",
    "    for img_name in os.listdir(image_dir):\n",
    "        img_path = os.path.join(image_dir, img_name)\n",
    "        with Image.open(img_path) as img:\n",
    "            if img.width > max_width:\n",
    "                max_width = img.width\n",
    "    return max_width\n",
    "\n",
    "def pad_images(image_dir, output_dir, max_width):\n",
    "    \"\"\" Pad all images in the specified directory to the maximum width. \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for img_name in os.listdir(image_dir):\n",
    "        img_path = os.path.join(image_dir, img_name)\n",
    "        with Image.open(img_path) as img:\n",
    "            new_img = Image.new('L', (max_width, 32), (0))  # Assuming white background (255)\n",
    "            new_img.paste(img, (0, 0))\n",
    "            new_img.save(os.path.join(output_dir, img_name))\n",
    "\n",
    "# Example usage:\n",
    "image_directory = \"Sentence_Dataset/images\"\n",
    "output_directory = \"Sentence_Dataset/pad\"\n",
    "max_width = find_max_width(image_directory)\n",
    "# pad_images(image_directory, output_directory, max_width)\n",
    "max_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 32, 1632, 1)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 32, 1632, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 16, 816, 32)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 16, 816, 64)       18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 408, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 8, 26112)          0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 8, 256)           26870784  \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 128)              164352    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               12900     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,066,852\n",
      "Trainable params: 27,066,852\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Reshape, Dense, LSTM, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def build_crnn_model(input_width, num_classes):\n",
    "    \"\"\" Build a CRNN model. \"\"\"\n",
    "    input_shape = (32, input_width, 1)  # Height is 32, width is variable, channel is 1 (grayscale)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Convolutional layers\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    # Reshape to prepare input for LSTM layers\n",
    "    new_shape = (-1, x.shape[2] * x.shape[3])  # Width times depth\n",
    "    x = Reshape(target_shape=new_shape)(x)\n",
    "\n",
    "    # RNN layer\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = Bidirectional(LSTM(64, return_sequences=False))(x)\n",
    "\n",
    "    # Output layer\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Assuming we have 100 different characters/classes including space and separators\n",
    "num_classes = 100\n",
    "model = build_crnn_model(max_width, num_classes)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         ...,\n",
       "         [0.5176471 ],\n",
       "         [0.00392157],\n",
       "         [0.        ]],\n",
       "\n",
       "        [[0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         ...,\n",
       "         [0.99215686],\n",
       "         [0.03137255],\n",
       "         [0.        ]],\n",
       "\n",
       "        [[0.        ],\n",
       "         [0.14117648],\n",
       "         [0.9137255 ],\n",
       "         ...,\n",
       "         [0.8235294 ],\n",
       "         [0.03921569],\n",
       "         [0.        ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.        ],\n",
       "         [0.13333334],\n",
       "         [0.95686275],\n",
       "         ...,\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ]],\n",
       "\n",
       "        [[0.        ],\n",
       "         [0.00392157],\n",
       "         [0.07058824],\n",
       "         ...,\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ]],\n",
       "\n",
       "        [[0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         ...,\n",
       "         [0.08235294],\n",
       "         [0.00784314],\n",
       "         [0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         ...,\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ]],\n",
       "\n",
       "        [[0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         ...,\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ]],\n",
       "\n",
       "        [[0.        ],\n",
       "         [0.        ],\n",
       "         [0.12941177],\n",
       "         ...,\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         ...,\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ]],\n",
       "\n",
       "        [[0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         ...,\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ]],\n",
       "\n",
       "        [[0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         ...,\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         ...,\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ]],\n",
       "\n",
       "        [[0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         ...,\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ]],\n",
       "\n",
       "        [[0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         ...,\n",
       "         [0.67058825],\n",
       "         [0.        ],\n",
       "         [0.        ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.        ],\n",
       "         [0.13333334],\n",
       "         [0.69803923],\n",
       "         ...,\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ]],\n",
       "\n",
       "        [[0.        ],\n",
       "         [0.00784314],\n",
       "         [0.23529412],\n",
       "         ...,\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ]],\n",
       "\n",
       "        [[0.        ],\n",
       "         [0.        ],\n",
       "         [0.02745098],\n",
       "         ...,\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         ...,\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ]],\n",
       "\n",
       "        [[0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         ...,\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ]],\n",
       "\n",
       "        [[0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         ...,\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         ...,\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ]],\n",
       "\n",
       "        [[0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         ...,\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ]],\n",
       "\n",
       "        [[0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         ...,\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         ...,\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ]],\n",
       "\n",
       "        [[0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         ...,\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ]],\n",
       "\n",
       "        [[0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         ...,\n",
       "         [0.29803923],\n",
       "         [0.29803923],\n",
       "         [0.        ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.        ],\n",
       "         [0.        ],\n",
       "         [0.04705882],\n",
       "         ...,\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ]],\n",
       "\n",
       "        [[0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         ...,\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ]],\n",
       "\n",
       "        [[0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         ...,\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         ...,\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ]],\n",
       "\n",
       "        [[0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         ...,\n",
       "         [0.00392157],\n",
       "         [0.        ],\n",
       "         [0.        ]],\n",
       "\n",
       "        [[0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         ...,\n",
       "         [0.38431373],\n",
       "         [0.00392157],\n",
       "         [0.        ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.        ],\n",
       "         [0.7921569 ],\n",
       "         [0.08627451],\n",
       "         ...,\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ]],\n",
       "\n",
       "        [[0.        ],\n",
       "         [0.26666668],\n",
       "         [0.00784314],\n",
       "         ...,\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ]],\n",
       "\n",
       "        [[0.        ],\n",
       "         [0.05098039],\n",
       "         [0.        ],\n",
       "         ...,\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ]]]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_images(image_dir, target_size=(32, 256)):\n",
    "    \"\"\" Load and preprocess images from the specified directory. \"\"\"\n",
    "    images = []\n",
    "    filenames = sorted(os.listdir(image_dir))  # Sort to ensure the order matches annotations\n",
    "    for filename in filenames:\n",
    "        img_path = os.path.join(image_dir, filename)\n",
    "        img = load_img(img_path, color_mode='grayscale', target_size=target_size)\n",
    "        img_array = img_to_array(img)\n",
    "        img_array = img_array / 255.0  # Normalize the images to [0, 1]\n",
    "        images.append(img_array)\n",
    "    return np.array(images)\n",
    "\n",
    "image_directory = 'Sentence_Dataset/images'\n",
    "images = load_images(image_directory)\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use encoded_annotations and images loaded from previous steps\n",
    "x_train, x_val, y_train, y_val = train_test_split(images, encoded_annotations, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture using TensorFlow/Keras\n",
    "def build_crnn_model(input_shape, num_classes):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    # Prepare output for RNN layers\n",
    "    new_shape = (-1, x.shape[2] * 64)  # Flatten and prepare for RNN\n",
    "    x = Reshape(target_shape=new_shape)(x)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = Bidirectional(LSTM(64, return_sequences=False))(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Calculate the number of classes (+1 for the padding character)\n",
    "num_classes = len(character_map) + 1\n",
    "input_shape = (32, 256, 1)  # Adjust depending on your image size and channels\n",
    "model = build_crnn_model(input_shape, num_classes)\n",
    "\n",
    "# Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\"<class \\'int\\'>\"})'})",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\SDP\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\SDP\\lib\\site-packages\\keras\\engine\\data_adapter.py:1083\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1080\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mcls\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ALL_ADAPTER_CLS \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mcan_handle(x, y)]\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m adapter_cls:\n\u001b[0;32m   1082\u001b[0m     \u001b[38;5;66;03m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[1;32m-> 1083\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1084\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to find data adapter that can handle \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1085\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(_type_name(x), _type_name(y))\n\u001b[0;32m   1086\u001b[0m     )\n\u001b[0;32m   1087\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(adapter_cls) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1088\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1089\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData adapters should be mutually exclusive for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1090\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhandling inputs. Found multiple adapters \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m to handle \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1091\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(adapter_cls, _type_name(x), _type_name(y))\n\u001b[0;32m   1092\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\"<class \\'int\\'>\"})'})"
     ]
    }
   ],
   "source": [
    "\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(image_path, target_size=(32, 256)):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.resize(img, target_size, interpolation=cv2.INTER_AREA)\n",
    "    img = img / 255.0  # Normalize pixel values\n",
    "    img = np.expand_dims(img, axis=-1)  # Add channel dimension\n",
    "    return img\n",
    "\n",
    "def encode_annotations(annotation):\n",
    "    char_to_num = {'-': 0, '#': 1}  # Define mapping for special characters\n",
    "    for idx, char in enumerate(set(annotation.replace('\\\\', '').replace(' ', '').replace('#', '')), start=2):\n",
    "        char_to_num[char] = idx  # Create a unique index for each Hindi character\n",
    "\n",
    "    # Split the annotation and encode\n",
    "    encoded = []\n",
    "    words = annotation.split(' ')\n",
    "    for word in words:\n",
    "        parts = word.split('\\\\')\n",
    "        word_encoded = [char_to_num[char] for char in parts if char in char_to_num]\n",
    "        encoded.extend(word_encoded)\n",
    "        encoded.append(char_to_num['#'])  # Add space between words as a token\n",
    "    return np.array(encoded[:-1])  # Remove the last space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_crnn_model(input_shape, num_classes):\n",
    "    input_img = layers.Input(shape=input_shape, name='image_input')\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Reshape(target_shape=(-1, x.shape[2] * x.shape[3]))(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
    "    output = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    model = models.Model(inputs=input_img, outputs=output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(base_directory, max_label_length):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    images_directory = os.path.join(base_directory, \"images\")\n",
    "    annotations_directory = os.path.join(base_directory, \"annotations\")\n",
    "    \n",
    "    for filename in os.listdir(images_directory):\n",
    "        if filename.endswith('.png'):\n",
    "            img_path = os.path.join(images_directory, filename)\n",
    "            annotation_path = os.path.join(annotations_directory, filename.replace('.png', '.txt'))\n",
    "            \n",
    "            img = load_and_preprocess_image(img_path)\n",
    "            with open(annotation_path, 'r', encoding='utf-8') as file:\n",
    "                annotation = file.read().strip()\n",
    "                label = encode_annotations(annotation)\n",
    "                \n",
    "                # Pad or truncate the label to the max_label_length\n",
    "                padded_label = np.pad(label, (0, max(max_label_length - len(label), 0)), mode='constant', constant_values=0)\n",
    "            \n",
    "            images.append(img)\n",
    "            labels.append(padded_label)\n",
    "    \n",
    "    return np.array(images), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (20000,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSentence_Dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m([\u001b[38;5;28mmax\u001b[39m(label) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Assuming labels are encoded as integers\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 24\u001b[0m, in \u001b[0;36mprepare_data\u001b[1;34m(base_directory)\u001b[0m\n\u001b[0;32m     21\u001b[0m         images\u001b[38;5;241m.\u001b[39mappend(img)\n\u001b[0;32m     22\u001b[0m         labels\u001b[38;5;241m.\u001b[39mappend(label)\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(images), \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (20000,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "directory = \"Sentence_Dataset\"\n",
    "max_label_length = 100  # Set this based on your data analysis\n",
    "images, labels = prepare_data(directory, max_label_length)\n",
    "num_classes = np.max(labels) + 1  # Assuming labels are encoded as integers\n",
    "\n",
    "# Continue with setting up and training your model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and Training\n",
    "model = build_crnn_model((32, 256, 1), num_classes)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(images, labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(image_path, target_size=(32, 256)):\n",
    "    # Read the image in grayscale\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    # Resize the image to the desired target size\n",
    "    img = cv2.resize(img, target_size, interpolation=cv2.INTER_AREA)\n",
    "    # Normalize pixel values to the range [0, 1]\n",
    "    img = img.astype('float32') / 255.0\n",
    "    # Add a channel dimension (for Keras/TensorFlow input requirements)\n",
    "    img = np.expand_dims(img, axis=-1)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_annotations(annotation, char_to_index, max_length):\n",
    "    # Initialize the encoded array with zeros (for padding)\n",
    "    encoded = np.zeros(max_length, dtype=int)\n",
    "    # Split the annotation into individual components\n",
    "    characters = annotation.split('\\\\')\n",
    "    # Encode each character using the provided mapping\n",
    "    for i, char in enumerate(characters):\n",
    "        if i < max_length:\n",
    "            encoded[i] = char_to_index.get(char, char_to_index['-'])  # Use '-' encoding for unknown characters\n",
    "    return encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(images_directory, annotations_directory, char_to_index, max_label_length):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # List all files in the images directory\n",
    "    image_files = os.listdir(images_directory)\n",
    "    for image_file in image_files:\n",
    "        if image_file.endswith('.png'):\n",
    "            # Corresponding annotation file\n",
    "            annotation_file = image_file.replace('.png', '.txt')\n",
    "            image_path = os.path.join(images_directory, image_file)\n",
    "            annotation_path = os.path.join(annotations_directory, annotation_file)\n",
    "\n",
    "            # Load and preprocess the image\n",
    "            img = load_and_preprocess_image(image_path)\n",
    "\n",
    "            # Read and encode the annotation\n",
    "            with open(annotation_path, 'r', encoding='utf-8') as file:\n",
    "                annotation = file.read().strip()\n",
    "                encoded_label = encode_annotations(annotation, char_to_index, max_label_length)\n",
    "            \n",
    "            # Append to the lists\n",
    "            images.append(img)\n",
    "            labels.append(encoded_label)\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    return np.array(images), np.array(labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SDP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
